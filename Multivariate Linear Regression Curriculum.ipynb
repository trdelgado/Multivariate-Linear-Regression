{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multivariate Linear Regression Curriculum.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pN-BcVyJPioy","colab_type":"text"},"source":["**Note: Please make your own copy of this notebook to run and execute, thank you!**\n","\n","1.   Go to the menu tab on the top left corner\n","2.   Click on \"File\"\n","3.   Under the File tab menu click on \"Save a copy in Drive...\""]},{"cell_type":"markdown","metadata":{"id":"5LITRUthPmGz","colab_type":"text"},"source":["# Introduction to Multivariate Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"RklwgxCsRjK_","colab_type":"text"},"source":["Multivariate regression analyzes the relationship between several independent variables and a dependent variable $y$. In fact, like univariate linear regression, multivariate linear regression allows us to find a boundary decision otherwise known as our line of best fit. This allows us make a prediction on unseen data."]},{"cell_type":"markdown","metadata":{"id":"CIQEp3dTBxu6","colab_type":"text"},"source":["For the remainder of this lesson, we'll cover how multivariate linear regression is used, how it makes predictions on multi-featured data, how to program it from scratch, and finally how to use exisiting data science libraries to perform the heavy lifting for us."]},{"cell_type":"markdown","metadata":{"id":"A_GgAwG9shZe","colab_type":"text"},"source":["[Multivariate Linear Regression Glossary](https://docs.google.com/document/d/1c8h1ejKDjlBoLCCY8c5Au0_ttEbbi_tc83EokgcsemY/edit):\n","\n","Provided as a list of terms and defintions used in the Multivariate Linear Regression lesson to help keep track of the content."]},{"cell_type":"markdown","metadata":{"id":"WIe3BLhr8GSf","colab_type":"text"},"source":["# Example of Multivariate Linar Regression - Selling a Used Car"]},{"cell_type":"markdown","metadata":{"id":"j4ArnYtyRjuh","colab_type":"text"},"source":["For the past several years, Bella has been using a 2007 Honda Civic for school and work. A senior high school student, she recently received wonderful news. She has been admitted to her first choice university! Unfortunately, after about 210,000 miles, her Honda Civic is now starting to require more regular checkups and maintenance. Considering the university is in a different town, She realized that she will be needing a more reliable car for longer commutes and decides to sell her car. Is there a machine learning tool that Bella can use to figure out how much she should sell her Honda Civic? Let's find out!"]},{"cell_type":"markdown","metadata":{"id":"SFVzy29goGA1","colab_type":"text"},"source":["While it's tempting sell the car based on the original price, potential buyers are looking for the best deal at an affordable price. Therefore, we need to consider that potential buyers want to pay less for a used car that might need more maintenance (otherwise Bella runs the risk of not being able to sell her car at all)."]},{"cell_type":"markdown","metadata":{"id":"WPO280-eomnj","colab_type":"text"},"source":["How can we determine a reasonable fair price to sell the car? In this case, we can look at historical data and determine that mileage and year manufactured both play a significant factor in the price.\n"]},{"cell_type":"markdown","metadata":{"id":"1sVdC1qczwtk","colab_type":"text"},"source":["Let's say that we managed to acquire historical data and determined that the average Honda Civic in the dataset at time of purchase were priced around 17,800 dollars. In addition, we also discover that the average Honda Civic has a yearly depreciation rate of 389 dollars, while for each mile driven, the price of the cars depreciates by roughly 5 cents."]},{"cell_type":"markdown","metadata":{"id":"8qr_NRFu_R5T","colab_type":"text"},"source":["From the data we gathered so far:\n","\n","*   Average Price of New Honda Civic: 17,800 dollars\n","\n","*   Yearly Depreciation Rate = -389 dollars\n","\n","*   Age of car = 2019 - 2007 = 12 years\n","\n","*  Mileage Depreciation Rate = -0.05 dollars\n","\n","*   Mileage = 210,000\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FWXGvV4kOt3b","colab_type":"text"},"source":["Great! Now, based on the information we gathered, let's calculate the estimated price for which Bella should sell her car."]},{"cell_type":"markdown","metadata":{"id":"U6ovrbxJKRjG","colab_type":"text"},"source":["$$Estimated\\ Price\\ of\\ Car = Original\\ Car\\ Price\\ - Yearly\\ Depreciation \\times Car\\ Age - Mileage\\ Depreciation \\times Car\\ Mileage$$\n","\n","$$ = 17800 - (389 \\times 12) - (0.05 \\times 210000)$$\n","\n","$$Estimated\\ Price\\ of\\ Car = 3082\\ dollars$$"]},{"cell_type":"markdown","metadata":{"id":"CAX_fbUCQavk","colab_type":"text"},"source":["Thus, if Bella wishes to sell her car at a competitive price, she can sell her car for roughly a little over $3000."]},{"cell_type":"markdown","metadata":{"id":"1QVgCBcv7zGe","colab_type":"text"},"source":[" **Note:**\n"," \n"," In reality, the price of a car can be influenced by many factors. We should also take into consideration the inflation, the local pricing , the history of the vehicle, its condition, and the model type.\n"," \n","In addition, take this example with a grain of salt since mileage and age of the car are highly correlated! Correlation is a number that indicates the degree of relationship between two features. This means that a very small change in one of those features can create big changes in the predictions made by our model. This is not good as the predictions made by our model can be incorrect.\n"]},{"cell_type":"markdown","metadata":{"id":"9Q7ZNsczPtoG","colab_type":"text"},"source":["# Making Predictions with Multivariate Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"7NvmxMx9QZxm","colab_type":"text"},"source":["[Multivariate Linear Regression](https://www.youtube.com/watch?v=aW4rB0Intd0): (1:24)\n","\n","As with univariate linear regression, we can perform multivariate linear regression if we have all the necessary information ahead of time to compute an estimated predicted value. The short video above is offered by Udacity and will briefly explain some of the terminology around multivariate linear regression and how it compares to univariate linear regression."]},{"cell_type":"markdown","metadata":{"id":"GDjWF9dGRhoE","colab_type":"text"},"source":["As seen in the video above, we can write a multivariate linear regression in polynomial form."]},{"cell_type":"markdown","metadata":{"id":"kn_hWZLcxHVD","colab_type":"text"},"source":["$$\\hat{y} = f(x) = w_nx_n+w_{n-1}x_{n-1}+\\ ...+\\ w_0$$"]},{"cell_type":"markdown","metadata":{"id":"ovawq9HLSw1m","colab_type":"text"},"source":["From the equation above, we can rewrite our formula to create a customized multivariate regression equation to predict the Honda Civic's price $y$.\n","\n","$$\\hat{y} = w_2 x_2 + w_1 x_1 + w_0$$\n","  "]},{"cell_type":"markdown","metadata":{"id":"h2uAoLFPw6dD","colab_type":"text"},"source":["Great! Now if we can name the different inputs used to predict in the equation above to predict the price of the car $y$. Notice that the equation becomes identical to the one we used to calculate the car's price as it should be.\n","\n"," * $w_2$  : A weight representing the yearly depreciation rate \n"," * $x_2$  : A variable representing the age of the car \n"," * $w_1$ : A weight representing the mileage depreciation rate\n"," * $x_1$  : A variable representing the miles on our car\n"," * $w_0$: Our intercept, also known as bias, which represents the original price of the car"]},{"cell_type":"markdown","metadata":{"id":"IreRhTW3Qcya","colab_type":"text"},"source":["[Vector Dot Products](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length): (9:09)\n","\n","We can also rewrite our generalized equation as two distinct sets: \n","* A set to contain our weight values $W$ \n","* A set to hold our feature values $X$ called vectors or matrices (for 2D vectors). \n","\n","This allows us to represent our equation in a more compact form when we are multiplying each weight with its respective input - also know as a dot product which Sal Khan will discuss in more detail if you would like to learn more. \n","\n","$$\\hat{y} = \\sum_{i=1}^{N}(w_i \\times x_i) = WX = Weights \\cdot Features$$\n"]},{"cell_type":"markdown","metadata":{"id":"tzgzpjIqQiwD","colab_type":"text"},"source":["**Example:**\n","$$Weights: W = [2, 1, 0]$$\n","$$Features: X = [1, 2, 3]$$\n"]},{"cell_type":"markdown","metadata":{"id":"GhZK1ADuyV_1","colab_type":"text"},"source":["$$\\hat{y} = WX = (w_1x_1) + (w_2x_2) + (w_3x_3)$$\n","\n","$$\\hat{y} = (2 \\times 1) + (1 \\times 2) + (0 \\times 3) = 4$$"]},{"cell_type":"markdown","metadata":{"id":"VW23S10nUHTb","colab_type":"text"},"source":["Great! We have covered some of the theory and math behind mulitvariate regression. Let's see how we can use the Numpy data science library to easily calculate a prediction based off the weights and features given to it."]},{"cell_type":"code","metadata":{"id":"A8YIY7-wtM0w","colab_type":"code","colab":{}},"source":["# Import the NumPy library\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9EjtO_NQmp0","colab_type":"code","colab":{}},"source":["# We begin with creating a custom multivariate/polynomial regression function\n","def reg_output(w,x):\n","  # Numpy's dot product method\n","  return np.dot(w,x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0TErOLSXYC8","colab_type":"text"},"source":["Fantastic! Now, let's test the regression model on the calculation we performed earlier to verify it works as intended."]},{"cell_type":"code","metadata":{"id":"JGJAhGT6Qo8x","colab_type":"code","colab":{}},"source":["# Define all the weights/slopes and input values to calculate regression output\n","# Weights\n","w = [2,1,0]\n","# Input values\n","x = [1,2,3]\n","print(reg_output(w,x))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"334L9TQ0P0ax","colab_type":"text"},"source":["# Fitting a Multivariate Linear Regression Line"]},{"cell_type":"markdown","metadata":{"id":"oe5fGm-XZrUl","colab_type":"text"},"source":["Usually, the first regression line that we obtain is a random line that does not represent the actual $Y_i$ values very well. As a result, just like univariate regression, it is important that we find a line that generalizes well over our data. However, unlike univariate regression, we won't have a specific set of formulas that can be used to automatically find the optimal weights for multivariate regression."]},{"cell_type":"markdown","metadata":{"id":"n3HKbVl4tGiZ","colab_type":"text"},"source":["To find a solution, we'll have to take an initial guess and figure out by how much the predicted values differs from the actual labeled data. From there, we'll update and tweak our weights $w_1$,  $w_2$, ..., $w_n$ to gradually improve the model's performance until we find an approximately good estimator."]},{"cell_type":"markdown","metadata":{"id":"MAVPACgEy0Fl","colab_type":"text"},"source":["### Cost Function"]},{"cell_type":"markdown","metadata":{"id":"rxDU65hMZpNV","colab_type":"text"},"source":["To understand how well our model is performing, we can use a **Cost Function**. A cost function measures the accuracy of a given model by comparing the predicted results with the expected outputs."]},{"cell_type":"markdown","metadata":{"id":"UndOptVdzH2k","colab_type":"text"},"source":["**Mean Squared Error**\n","\n","One such example of a cost function is the **mean squared error** which is the average of sum of squared errors used in univariate regression but taken one step further to average the square error results. To illustrate what mean squared error is, imagine playing with a paintball to reach a yellow spot at the center of a wall. The mean square error represent the average squared distance from where the paintball hits the wall from the center. Similarly to this example, the mean squared error tells us how close our regression line was to reaching the true line representing the observed $Y_i$ values. \n","\n","To compute the **Mean Squared Error** we can follow these steps:\n","*   Find the difference between the observed $Y_i$ value and the predicted $ \\hat{Y_i}$ value.\n","*   Square the difference.\n","*   Sum and then average the differences for every datapoint.\n","\n","The mathematical equation of the mean squared error can be expressed as follows "]},{"cell_type":"markdown","metadata":{"id":"NIFPqpl2zPD8","colab_type":"text"},"source":["$$Mean\\ Squared\\ Error = \\frac{1}{N}\\ \\sum_{i=1}^{N}(Y_i - \\hat{Y_i})^2$$\n"]},{"cell_type":"markdown","metadata":{"id":"F3fUuzWPQLTU","colab_type":"text"},"source":["* $N$ is the number of data points\n","* $Y_i$    represents the observed values\n","* $\\hat{Y_i} $ represents the predicted values"]},{"cell_type":"markdown","metadata":{"id":"TdSQZl8NOxv4","colab_type":"text"},"source":["**Example:**\n","\n","$$\\hat{y} = \\{2,\\ 4,\\ 6,\\ 8,\\ 10\\}$$\n","\n","$$y = \\{2.1,\\ 3.9,\\ 5.8,\\ 7.9,\\ 10.2\\}$$\n","\n","$$MSE = \\frac{(2.1-2)^2 + (3.9-4)^2 + (5.8-6)^2 + (7.9-8)^2 + (10.2-10)^2}{5} = 0.021\\bar{9}$$"]},{"cell_type":"markdown","metadata":{"id":"70zxa5Gkb2vr","colab_type":"text"},"source":["Now let's convert this equation into code."]},{"cell_type":"code","metadata":{"id":"lj364hOLb6tc","colab_type":"code","colab":{}},"source":["def mean_squared_errors(y, y_predicted):\n","  # Ensure the size of y and y_predicted are the same\n","  assert(len(y) == len(y_predicted))\n","\n","  # No need for loops since Numpy automatically calculates the sse for each pair of elements\n","  sse = ((y - y_predicted)**2)\n","  \n","  # To get the total we can call Numpy's sum method\n","  total = sse.sum()\n","  \n","  # Return the mean squared error result\n","  result = total/len(y)\n","  return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYHARfkAQCpS","colab_type":"text"},"source":["Great! Now, let's verify if the above code works with the example we calculated."]},{"cell_type":"code","metadata":{"id":"0FEgkOkjQCJv","colab_type":"code","colab":{}},"source":["# Hypothetical predictive values generated by model\n","y_pred = np.array([2, 4, 6, 8, 10])\n","# Labeled data outcomes to compare how well our model is performing (with some amount of random noise)\n","y_test = np.array([2.1, 3.9, 5.8, 7.9, 10.2])\n","\n","# Output the sum of squared errors between the predicted values of the model and the actual data\n","print(mean_squared_errors(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1fXtcgV0Ue8","colab_type":"text"},"source":["### Minimizing Cost with the Update Rule"]},{"cell_type":"markdown","metadata":{"id":"9dPT4SC90R08","colab_type":"text"},"source":["Amazing! We now have an equation that allows us to find the total error that our model makes when predicting a $Y$ value. But how can we use it to reduce the errors in our model? Well, one approach we can use is to update the weights such that the the overall error is gradually reduced. In other words, we gradually reduce the difference between the actual $y_i$ values and the predicted  $\\hat{y_i}$ values.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rVtkn31EQwoo","colab_type":"text"},"source":["**Update Rule:**\n","\n","\n","\n","Since we start with a random line for our linear regression model, we'll use the MSE to iterately update our weights to reduce our overall error. Let's start with a simple update rule where we update each weight based on the previous value and take into consideration the direction we'll need to move it. This direction to move the weight is called a ***gradient***  and is symbolized by $\\Delta w$. \n","\n","$$w_i \\leftarrow w_i -\\ \\Delta w$$\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"YNHqGT9F-247","colab_type":"text"},"source":["Another way to represent our update rule with $n$ set of weight vector parameters is to update each weight based on the direction we need to tune them:\n","\n","$$\\begin{bmatrix} \n","w_{new\\ value_1} \\\\\n","w_{new\\ value_2} \\\\\n","w_{new\\ value_3} \\\\\n","... \\\\\n","w_{new\\ value_n} \n","\\end{bmatrix}\n","=\n","\\begin{bmatrix} \n","w_{old\\ value_1} \\\\\n","w_{old\\ value_2} \\\\\n","w_{old\\ value_3} \\\\\n","... \\\\\n","w_{old\\ value_n}\n","\\end{bmatrix}\n","-\n","\\begin{bmatrix} \n","\\Delta w_1 \\\\\n","\\Delta w_2 \\\\\n","\\Delta w_3 \\\\\n","... \\\\\n","\\Delta w_n\n","\\end{bmatrix}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"6jHHBCZh8xf_","colab_type":"text"},"source":["Here is the equivalent in Python code."]},{"cell_type":"code","metadata":{"id":"H1GXvjRQr53l","colab_type":"code","colab":{}},"source":["# Normally we'd have to update each value individually using loops\n","# However if we use NumPy it will automatically perform vector arithmetic for us\n","weights = weights - delta_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u12rjFrFaC8W","colab_type":"text"},"source":["**Learning Rate:**\n","\n","Let's assume we already have our $\\Delta w$ values to update our parameters (we'll discuss how to find them in a bit). How far should we update our weight values in a single iteration? If we have a large number, we might overcompensate and make our model worse or on the other hand if it's too small we'll have to make a lot of iterations before our model finds the ideal weight parameter. To help us we'll create a variable called the *learning rate* $\\alpha$ to help us adjust the weights. Note: The learning rate is defined before training begins and usually starts with a default of 0.01 but can be changed based off how much learning is taking place.\n","\n","$$w_i \\leftarrow w_i -\\ \\alpha \\Delta w$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ymM8oVzOBnPr","colab_type":"text"},"source":["**Update Rule to Minimize our Loss:**"]},{"cell_type":"markdown","metadata":{"id":"i_anQwbABp-9","colab_type":"text"},"source":["Fantastic! Now we'll need to figure out which direction to tune our weights based of our errors generated. Earlier we discussed that our approach is to minimize our cost function - the mean squared error. "]},{"cell_type":"markdown","metadata":{"id":"f-svIXZhBumP","colab_type":"text"},"source":["To illustrate how to minimize our error, imagine the cost function as a mountain where the altitude is related to how high our error is. While a great hike, your goal is to reach the bottom of the mountain. Reaching the bottom is equivalent to reaching the lowest point of the cost function curve. Once we reach the \"bottom\" of a cost function, we have minimized our error. With every step taken (every iteration), you look around and update your path by deciding which direction to follow next."]},{"cell_type":"markdown","metadata":{"id":"s0UNWlxGQzfE","colab_type":"text"},"source":["To determine the *gradient* $\\Delta w$, we'll use a mathematical magic tool called the derivative which represents the slope of a function or how steep a \"hill\" of the cost function is. Under the hood, you are using gradients to find the best slope or the best \"direction\" to move down. (*Note: Derivatives are part of Calculus, however, you will not need to know the details  to follow the remainder of this lesson*).\n"]},{"cell_type":"markdown","metadata":{"id":"FS5UL1s9B3DG","colab_type":"text"},"source":["Using gradients will help us find a formula that minimizes our overall error. So let's replace our $\\Delta w$ with the derivative of our cost function $Loss(W)$:\n","\n","$$w_i \\leftarrow w_i - \\alpha \\frac{\\delta}{\\delta w_i}Loss(W)$$"]},{"cell_type":"code","metadata":{"id":"wUt51PtQsFPO","colab_type":"code","colab":{}},"source":["# Dummy code\n","weight = weight - alpha * weight_gradient(weight)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ai5Q2h5jQ3wl","colab_type":"text"},"source":["Now let's replace our cost function $Loss(w)$ with our cost function which we mention earlier is the mean of squared errors $(y - \\hat{y})^2$ :\n","\n","$$w_i \\leftarrow w_i - \\alpha \\frac{\\delta}{\\delta w_i}(y - \\hat{y})^2$$"]},{"cell_type":"markdown","metadata":{"id":"rGnpYQju8vGe","colab_type":"text"},"source":["While we won't get into all the proofs,  we'll use calculus to reduce our update rule to the following:\n","\n","$$w_i \\leftarrow w_i +\\ \\alpha\\ (y - \\hat{y})x$$"]},{"cell_type":"code","metadata":{"id":"NyKRhSE3sI0r","colab_type":"code","colab":{}},"source":["weight = weight + alpha * (true_y - predicted_y) * x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USc6PSCvQ8Zr","colab_type":"text"},"source":["When our weights for multiple examples of size N we modify the formula as follows:\n","\n","$$w_i \\leftarrow w_i +\\alpha\\sum_{i=0}^{N}(y_i - \\hat{y})x_i$$"]},{"cell_type":"markdown","metadata":{"id":"coHFy09d-wJE","colab_type":"text"},"source":["Great! So we have our labels $Y$ and features $X$ from our data, but how do we find $\\hat{y}$ if this was our original goal all along? Recall $\\hat{y}$ represents our regression model so we can just replace it with our initial set of weight parameters $W$ and iteratively tune and improve it:\n","\n","$$w_i \\leftarrow w_i +\\alpha\\sum_{i=0}^{N}(y_i - regression(W, X))x_i$$"]},{"cell_type":"markdown","metadata":{"id":"BU_z_dEaWJNl","colab_type":"text"},"source":["Here is the weight update rule in Python code which will update all of our weight values simultaneously."]},{"cell_type":"code","metadata":{"id":"YrCFDI6KRF1H","colab_type":"code","colab":{}},"source":["# Update all our weights for each set of data points\n","def weight_update_rule(x, y, weights, learning_rate):\n","  weights += learning_rate * (y - reg_output(weights, x))*x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usZnKJ85FWkq","colab_type":"text"},"source":["### Convergence using Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"clC3A9snr0Vv","colab_type":"text"},"source":[" [Convergence using Gradient Descent](https://www.youtube.com/watch?v=BEC0uH1fuGU): (1:02)\n"," \n","When teaching our model how do we know how long to train it or when we found an optimal solution? Is there another tool in our magic box to help us minimize the cost function? Well, we have more fantastic news! Luis Serrano will once again walk us through a popular tool to help us minimize our error called **Gradient Descent**."]},{"cell_type":"markdown","metadata":{"id":"pABOZ9vSC3qw","colab_type":"text"},"source":["As seen in the Mountain Errorest example, the gradient descent algorithm lead us to the bottom of that mountain. Once we reach the bottom this indicates the model has *converged* with the training data. This means that we have reduced as much error as possible which should minimize difference between the expected and the predicted value."]},{"cell_type":"markdown","metadata":{"id":"ClKHUodcRJKu","colab_type":"text"},"source":["Now let's take what we learned from Gradient Descent to write some step we can use to iteratively update our weights:\n","\n","\n","$$\n","\\text{loop until converge:}\n","\\\\\\qquad\\text{for point in data:}\n","\\\\ \\qquad\\qquad\\qquad\\qquad w_i \\leftarrow w_i +\\ \\alpha\\ (y - \\hat{y})x\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pHYwIF62i7fB","colab_type":"text"},"source":["**Epochs**\n","\n","In order to track the number of times the learning algorithm works through the entire dataset, we will be using **epochs**. One epoch  indicates that our learning algorithm has worked through the dataset once. Usually, the number of epochs is large as the learning algorithm has to run through the dataset many times to minimize our error."]},{"cell_type":"markdown","metadata":{"id":"AXAWgzEHD0qV","colab_type":"text"},"source":["Now let's put theory and math into concrete code by creating our gradient descent function."]},{"cell_type":"code","metadata":{"id":"YXXWI03PRNNn","colab_type":"code","colab":{}},"source":["def gradient_descent(features, targets, epochs, learning_rate):\n","  # Initialize small random weights based on number of features\n","  n_records, n_features = features.shape\n","  # Convert our weights into a standard scale of the model to learn faster and more efficiently\n","  weights = np.random.normal(scale=1/n_features**0.5, size=n_features)\n","  \n","  # Train for number of epochs\n","  for e in range(epochs):\n","    # Update weights for each data point\n","    for x, y in zip(features, targets):\n","      # Update weights based on the Update Rule\n","      weight_update_rule(x, y, weights, learning_rate)\n","    # Display weight after a number of epochs has been complete\n","    if e%10 == 0:\n","      print(\"Epoch: \", e)\n","      print(\"Weights: \", weights)\n","  return weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6E2fgVAcjabg","colab_type":"text"},"source":["Great! We will test our gradient descent algorithm with some synthetic data with a default of 100 epochs and a small learning rate of 0.001. Let's see how well it learns!"]},{"cell_type":"code","metadata":{"id":"7iTbhiwHRPER","colab_type":"code","colab":{}},"source":["np.random.seed(44)\n","x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1,1)\n","y = np.array([1.1, 1.9, 2.8, 4, 5.2, 5.8, 6.9, 8.1, 9, 9.9]).reshape(-1,1)\n","epochs = 100\n","learning_rate = 0.001\n","gradient_descent(x, y, epochs, learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_v6rn0JP6ns","colab_type":"text"},"source":["# Building Our Multivariate Linear Regression Model"]},{"cell_type":"markdown","metadata":{"id":"SNFhAPT6RSNM","colab_type":"text"},"source":["Now let's put all our previous code together to make an improved regression model."]},{"cell_type":"code","metadata":{"id":"LBeExobVRRpC","colab_type":"code","colab":{}},"source":["# Define the multivariate linear regression\n","class multivariate_regression_model:\n","  # Initialize our hyper and weight parameters\n","  def __init__(self, lr=0.01, ep=1000):\n","    self.learning_rate = lr\n","    self.epochs = ep\n","    self.weights = None\n","  \n","  # Train regression model based of size and shape of the data\n","  def train(self, features, targets):\n","    # Initialize small random weights based on number of features\n","    n_records, n_features = features.shape\n","    self.weights = np.random.normal(scale=1/n_features**0.5, size=n_features)\n","  \n","    # Train for number of epochs\n","    for e in range(self.epochs):\n","      # Update weights for each data point\n","      for x, y in zip(features, targets):\n","        # Update Rule\n","        weight_update_rule(x, y, self.weights, self.learning_rate)\n","\t\n","  # Return the predicted value based off the feature and weight parameters\n","  def predict(self, x):\n","    return reg_output(self.weights, x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3KHTp_xP9_B","colab_type":"text"},"source":["# Evaluating Our Multivariate Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"VbWOhsk9RYwE","colab_type":"text"},"source":["Now that our multivariate regression model is defined, let's test it on some small data to see if it works as intended."]},{"cell_type":"code","metadata":{"id":"yLUVUmhlRauS","colab_type":"code","colab":{}},"source":["# Dataset to train modeld\n","features = np.array([[1,2],[2,3],[3,5],[2,4],[2,3],[1,6],[1,5],[3,1],[4,2],[2,2]])\n","# z = 1x + 2y\n","labels = np.array([5, 8, 13, 10, 8, 13, 11, 5, 8, 6])\n","\n","# Initialize our model\n","mult_reg = multivariate_regression_model()\n","\n","# Train our model with the data\n","mult_reg.train(features, labels)\n","\n","# Make a prediction\n","# z = (1)(3) + (2)(3) = 3 + 6 = 9\n","print(mult_reg.predict([3,3]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UI9OLloJboCE","colab_type":"text"},"source":["# Linear Regression with Scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"-jNs6hpDb4lW","colab_type":"text"},"source":["Now that we have seen how to build a multivariate linear regression from scratch, let's learn how to implement multivariate linear regression using the [*Scikit-Learn*](https://scikit-learn.org/stable/index.html) library. The *Scikit-Learn* (also known as Sklearn) library comes with pre-built algorithms and is commonly used in machine learning to analyze data. It also comes with some popular datasets such as the [Iris Flowers](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) dataset (which we will not use) but contains information about characteristics of Iris flowers such as the sepal length and the petal length.\n"]},{"cell_type":"markdown","metadata":{"id":"Y1ZXbu00D7zu","colab_type":"text"},"source":["Using the [linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)  from the *Skicit-Learn* library, let's see how to rapidly implement multivariate linear regression to make a prediction. \n"]},{"cell_type":"code","metadata":{"id":"_Q0qvLc0ffz0","colab_type":"code","colab":{}},"source":["#1. We import the linear regression model from Skicit-Learn library\n","from sklearn.linear_model import LinearRegression\n","\n","#2. We load a toy dataset X as our training data\n","X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n","\n","#3. We define our target values y = 1 * x_0 + 2 * x_1 + 3\n","y = np.dot(X, np.array([1, 2])) + 3\n","\n","#4. We fit our linear model\n","regression = LinearRegression().fit(X, y)\n","\n","#5. We can view the intercept of our model\n","intercept = regression.intercept_ \n","print(\"The intercept for our model is {}\".format(intercept))\n","\n","#6. We can find out the accuracy of our model using R^2 score. \n","# Keep in mind that the best possible score is 1.0\n","score = regression.score(X, y)\n","print(\"The score for our model is {}\".format(score))\n","\n","#7. We can make a prediction for a new value\n","regression.predict(np.array([[3, 5]]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZhL_CcLQCiI","colab_type":"text"},"source":["# Summary"]},{"cell_type":"markdown","metadata":{"id":"SfvM2ikQmnV3","colab_type":"text"},"source":["In this lesson, we discussed how multivariate linear regression is a mathematical tool used to find a relationship between two or more independent variables $X_i$ to 'explain' or predict a dependent variable $Y$. Unlike univariate linear regression, we do not have a set of default equations to find the optimal parameter weights so we have to tune and iterate them to find an approximate solution."]},{"cell_type":"markdown","metadata":{"id":"GJ55qsc6Esoy","colab_type":"text"},"source":["In order to calculate the accuracy of our model and determine the difference between an actual $Y_i$ value and the predicted $\\hat{Y_i}$ value, we use a **cost function** using the **Means Squared Error** method to find out how well our model is performing. Last, we used the **Update Rule** and **Gradient Descent** algorithm to update our weights so we reduce as much errors in our model as possible."]}]}